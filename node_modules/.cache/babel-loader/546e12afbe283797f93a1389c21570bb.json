{"ast":null,"code":"\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.tokenMatcher = exports.createTokenInstance = exports.EOF = exports.createToken = exports.hasTokenLabel = exports.tokenName = exports.tokenLabel = void 0;\n\nvar utils_1 = require(\"@chevrotain/utils\");\n\nvar lexer_public_1 = require(\"./lexer_public\");\n\nvar tokens_1 = require(\"./tokens\");\n\nfunction tokenLabel(tokType) {\n  if (hasTokenLabel(tokType)) {\n    return tokType.LABEL;\n  } else {\n    return tokType.name;\n  }\n}\n\nexports.tokenLabel = tokenLabel;\n\nfunction tokenName(tokType) {\n  return tokType.name;\n}\n\nexports.tokenName = tokenName;\n\nfunction hasTokenLabel(obj) {\n  return (0, utils_1.isString)(obj.LABEL) && obj.LABEL !== \"\";\n}\n\nexports.hasTokenLabel = hasTokenLabel;\nvar PARENT = \"parent\";\nvar CATEGORIES = \"categories\";\nvar LABEL = \"label\";\nvar GROUP = \"group\";\nvar PUSH_MODE = \"push_mode\";\nvar POP_MODE = \"pop_mode\";\nvar LONGER_ALT = \"longer_alt\";\nvar LINE_BREAKS = \"line_breaks\";\nvar START_CHARS_HINT = \"start_chars_hint\";\n\nfunction createToken(config) {\n  return createTokenInternal(config);\n}\n\nexports.createToken = createToken;\n\nfunction createTokenInternal(config) {\n  var pattern = config.pattern;\n  var tokenType = {};\n  tokenType.name = config.name;\n\n  if (!(0, utils_1.isUndefined)(pattern)) {\n    tokenType.PATTERN = pattern;\n  }\n\n  if ((0, utils_1.has)(config, PARENT)) {\n    throw \"The parent property is no longer supported.\\n\" + \"See: https://github.com/chevrotain/chevrotain/issues/564#issuecomment-349062346 for details.\";\n  }\n\n  if ((0, utils_1.has)(config, CATEGORIES)) {\n    // casting to ANY as this will be fixed inside `augmentTokenTypes``\n    tokenType.CATEGORIES = config[CATEGORIES];\n  }\n\n  (0, tokens_1.augmentTokenTypes)([tokenType]);\n\n  if ((0, utils_1.has)(config, LABEL)) {\n    tokenType.LABEL = config[LABEL];\n  }\n\n  if ((0, utils_1.has)(config, GROUP)) {\n    tokenType.GROUP = config[GROUP];\n  }\n\n  if ((0, utils_1.has)(config, POP_MODE)) {\n    tokenType.POP_MODE = config[POP_MODE];\n  }\n\n  if ((0, utils_1.has)(config, PUSH_MODE)) {\n    tokenType.PUSH_MODE = config[PUSH_MODE];\n  }\n\n  if ((0, utils_1.has)(config, LONGER_ALT)) {\n    tokenType.LONGER_ALT = config[LONGER_ALT];\n  }\n\n  if ((0, utils_1.has)(config, LINE_BREAKS)) {\n    tokenType.LINE_BREAKS = config[LINE_BREAKS];\n  }\n\n  if ((0, utils_1.has)(config, START_CHARS_HINT)) {\n    tokenType.START_CHARS_HINT = config[START_CHARS_HINT];\n  }\n\n  return tokenType;\n}\n\nexports.EOF = createToken({\n  name: \"EOF\",\n  pattern: lexer_public_1.Lexer.NA\n});\n(0, tokens_1.augmentTokenTypes)([exports.EOF]);\n\nfunction createTokenInstance(tokType, image, startOffset, endOffset, startLine, endLine, startColumn, endColumn) {\n  return {\n    image: image,\n    startOffset: startOffset,\n    endOffset: endOffset,\n    startLine: startLine,\n    endLine: endLine,\n    startColumn: startColumn,\n    endColumn: endColumn,\n    tokenTypeIdx: tokType.tokenTypeIdx,\n    tokenType: tokType\n  };\n}\n\nexports.createTokenInstance = createTokenInstance;\n\nfunction tokenMatcher(token, tokType) {\n  return (0, tokens_1.tokenStructuredMatcher)(token, tokType);\n}\n\nexports.tokenMatcher = tokenMatcher;","map":{"version":3,"mappings":";;;;;;;AAAA;;AACA;;AACA;;AAGA,SAAgBA,UAAhB,CAA2BC,OAA3B,EAA6C;AAC3C,MAAIC,aAAa,CAACD,OAAD,CAAjB,EAA4B;AAC1B,WAAOA,OAAO,CAACE,KAAf;AACD,GAFD,MAEO;AACL,WAAOF,OAAO,CAACG,IAAf;AACD;AACF;;AANDC;;AAQA,SAAgBC,SAAhB,CAA0BL,OAA1B,EAA4C;AAC1C,SAAOA,OAAO,CAACG,IAAf;AACD;;AAFDC;;AAIA,SAAgBH,aAAhB,CAA8BK,GAA9B,EAA4C;AAC1C,SAAO,sBAAeA,GAAI,CAACJ,KAApB,KAAoCI,GAAI,CAACJ,KAAL,KAAe,EAA1D;AACD;;AAFDE;AAIA,IAAMG,MAAM,GAAG,QAAf;AACA,IAAMC,UAAU,GAAG,YAAnB;AACA,IAAMN,KAAK,GAAG,OAAd;AACA,IAAMO,KAAK,GAAG,OAAd;AACA,IAAMC,SAAS,GAAG,WAAlB;AACA,IAAMC,QAAQ,GAAG,UAAjB;AACA,IAAMC,UAAU,GAAG,YAAnB;AACA,IAAMC,WAAW,GAAG,aAApB;AACA,IAAMC,gBAAgB,GAAG,kBAAzB;;AAEA,SAAgBC,WAAhB,CAA4BC,MAA5B,EAAgD;AAC9C,SAAOC,mBAAmB,CAACD,MAAD,CAA1B;AACD;;AAFDZ;;AAIA,SAASa,mBAAT,CAA6BD,MAA7B,EAAiD;AAC/C,MAAME,OAAO,GAAGF,MAAM,CAACE,OAAvB;AAEA,MAAMC,SAAS,GAAmB,EAAlC;AACAA,WAAS,CAAChB,IAAV,GAAiBa,MAAM,CAACb,IAAxB;;AAEA,MAAI,CAAC,yBAAYe,OAAZ,CAAL,EAA2B;AACzBC,aAAS,CAACC,OAAV,GAAoBF,OAApB;AACD;;AAED,MAAI,iBAAIF,MAAJ,EAAYT,MAAZ,CAAJ,EAAyB;AACvB,UACE,kDACA,8FAFF;AAID;;AAED,MAAI,iBAAIS,MAAJ,EAAYR,UAAZ,CAAJ,EAA6B;AAC3B;AACAW,aAAS,CAACX,UAAV,GAA4BQ,MAAM,CAACR,UAAD,CAAlC;AACD;;AAED,kCAAkB,CAACW,SAAD,CAAlB;;AAEA,MAAI,iBAAIH,MAAJ,EAAYd,KAAZ,CAAJ,EAAwB;AACtBiB,aAAS,CAACjB,KAAV,GAAkBc,MAAM,CAACd,KAAD,CAAxB;AACD;;AAED,MAAI,iBAAIc,MAAJ,EAAYP,KAAZ,CAAJ,EAAwB;AACtBU,aAAS,CAACV,KAAV,GAAkBO,MAAM,CAACP,KAAD,CAAxB;AACD;;AAED,MAAI,iBAAIO,MAAJ,EAAYL,QAAZ,CAAJ,EAA2B;AACzBQ,aAAS,CAACR,QAAV,GAAqBK,MAAM,CAACL,QAAD,CAA3B;AACD;;AAED,MAAI,iBAAIK,MAAJ,EAAYN,SAAZ,CAAJ,EAA4B;AAC1BS,aAAS,CAACT,SAAV,GAAsBM,MAAM,CAACN,SAAD,CAA5B;AACD;;AAED,MAAI,iBAAIM,MAAJ,EAAYJ,UAAZ,CAAJ,EAA6B;AAC3BO,aAAS,CAACP,UAAV,GAAuBI,MAAM,CAACJ,UAAD,CAA7B;AACD;;AAED,MAAI,iBAAII,MAAJ,EAAYH,WAAZ,CAAJ,EAA8B;AAC5BM,aAAS,CAACN,WAAV,GAAwBG,MAAM,CAACH,WAAD,CAA9B;AACD;;AAED,MAAI,iBAAIG,MAAJ,EAAYF,gBAAZ,CAAJ,EAAmC;AACjCK,aAAS,CAACL,gBAAV,GAA6BE,MAAM,CAACF,gBAAD,CAAnC;AACD;;AAED,SAAOK,SAAP;AACD;;AAEYf,cAAMW,WAAW,CAAC;AAAEZ,MAAI,EAAE,KAAR;AAAee,SAAO,EAAEG,qBAAMC;AAA9B,CAAD,CAAjB;AACb,gCAAkB,CAAClB,WAAD,CAAlB;;AAEA,SAAgBmB,mBAAhB,CACEvB,OADF,EAEEwB,KAFF,EAGEC,WAHF,EAIEC,SAJF,EAKEC,SALF,EAMEC,OANF,EAOEC,WAPF,EAQEC,SARF,EAQmB;AAEjB,SAAO;AACLN,SAAK,OADA;AAELC,eAAW,aAFN;AAGLC,aAAS,WAHJ;AAILC,aAAS,WAJJ;AAKLC,WAAO,SALF;AAMLC,eAAW,aANN;AAOLC,aAAS,WAPJ;AAQLC,gBAAY,EAAQ/B,OAAQ,CAAC+B,YARxB;AASLZ,aAAS,EAAEnB;AATN,GAAP;AAWD;;AArBDI;;AAuBA,SAAgB4B,YAAhB,CAA6BC,KAA7B,EAA4CjC,OAA5C,EAA8D;AAC5D,SAAO,qCAAuBiC,KAAvB,EAA8BjC,OAA9B,CAAP;AACD;;AAFDI","names":["tokenLabel","tokType","hasTokenLabel","LABEL","name","exports","tokenName","obj","PARENT","CATEGORIES","GROUP","PUSH_MODE","POP_MODE","LONGER_ALT","LINE_BREAKS","START_CHARS_HINT","createToken","config","createTokenInternal","pattern","tokenType","PATTERN","lexer_public_1","NA","createTokenInstance","image","startOffset","endOffset","startLine","endLine","startColumn","endColumn","tokenTypeIdx","tokenMatcher","token"],"sources":["/Users/arbus/Documents/SpaceInBrowser/node_modules/chevrotain/src/scan/tokens_public.ts"],"sourcesContent":["import { has, isString, isUndefined } from \"@chevrotain/utils\"\nimport { Lexer } from \"./lexer_public\"\nimport { augmentTokenTypes, tokenStructuredMatcher } from \"./tokens\"\nimport { IToken, ITokenConfig, TokenType } from \"@chevrotain/types\"\n\nexport function tokenLabel(tokType: TokenType): string {\n  if (hasTokenLabel(tokType)) {\n    return tokType.LABEL\n  } else {\n    return tokType.name\n  }\n}\n\nexport function tokenName(tokType: TokenType): string {\n  return tokType.name\n}\n\nexport function hasTokenLabel(obj: TokenType): boolean {\n  return isString((<any>obj).LABEL) && (<any>obj).LABEL !== \"\"\n}\n\nconst PARENT = \"parent\"\nconst CATEGORIES = \"categories\"\nconst LABEL = \"label\"\nconst GROUP = \"group\"\nconst PUSH_MODE = \"push_mode\"\nconst POP_MODE = \"pop_mode\"\nconst LONGER_ALT = \"longer_alt\"\nconst LINE_BREAKS = \"line_breaks\"\nconst START_CHARS_HINT = \"start_chars_hint\"\n\nexport function createToken(config: ITokenConfig): TokenType {\n  return createTokenInternal(config)\n}\n\nfunction createTokenInternal(config: ITokenConfig): TokenType {\n  const pattern = config.pattern\n\n  const tokenType: TokenType = <any>{}\n  tokenType.name = config.name\n\n  if (!isUndefined(pattern)) {\n    tokenType.PATTERN = pattern\n  }\n\n  if (has(config, PARENT)) {\n    throw (\n      \"The parent property is no longer supported.\\n\" +\n      \"See: https://github.com/chevrotain/chevrotain/issues/564#issuecomment-349062346 for details.\"\n    )\n  }\n\n  if (has(config, CATEGORIES)) {\n    // casting to ANY as this will be fixed inside `augmentTokenTypes``\n    tokenType.CATEGORIES = <any>config[CATEGORIES]\n  }\n\n  augmentTokenTypes([tokenType])\n\n  if (has(config, LABEL)) {\n    tokenType.LABEL = config[LABEL]\n  }\n\n  if (has(config, GROUP)) {\n    tokenType.GROUP = config[GROUP]\n  }\n\n  if (has(config, POP_MODE)) {\n    tokenType.POP_MODE = config[POP_MODE]\n  }\n\n  if (has(config, PUSH_MODE)) {\n    tokenType.PUSH_MODE = config[PUSH_MODE]\n  }\n\n  if (has(config, LONGER_ALT)) {\n    tokenType.LONGER_ALT = config[LONGER_ALT]\n  }\n\n  if (has(config, LINE_BREAKS)) {\n    tokenType.LINE_BREAKS = config[LINE_BREAKS]\n  }\n\n  if (has(config, START_CHARS_HINT)) {\n    tokenType.START_CHARS_HINT = config[START_CHARS_HINT]\n  }\n\n  return tokenType\n}\n\nexport const EOF = createToken({ name: \"EOF\", pattern: Lexer.NA })\naugmentTokenTypes([EOF])\n\nexport function createTokenInstance(\n  tokType: TokenType,\n  image: string,\n  startOffset: number,\n  endOffset: number,\n  startLine: number,\n  endLine: number,\n  startColumn: number,\n  endColumn: number\n): IToken {\n  return {\n    image,\n    startOffset,\n    endOffset,\n    startLine,\n    endLine,\n    startColumn,\n    endColumn,\n    tokenTypeIdx: (<any>tokType).tokenTypeIdx,\n    tokenType: tokType\n  }\n}\n\nexport function tokenMatcher(token: IToken, tokType: TokenType): boolean {\n  return tokenStructuredMatcher(token, tokType)\n}\n"]},"metadata":{},"sourceType":"script"}